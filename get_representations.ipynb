{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Representations for Phrasal VERBS\n",
    "In this notebook, I generate all the necessary embeddings for each element of the phrasal verb:\n",
    "\n",
    "1. the whole phrasal verb\n",
    "2. the verb only\n",
    "3. all non-verb elements in the phrasal verb\n",
    "\n",
    "This notebook ~28 minutes to run (currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from miniconsbatched import generate_representations\n",
    "\n",
    "from minicons import cwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('data/pvc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating verbs and non-verb elements\n",
    "data['verb_string_only'] = data['verbs_fixed'].apply(lambda x: eval(x)[0])\n",
    "data['non_verbs_only'] = data['verbs_fixed'].apply(lambda x: ' '.join(eval(x)[1:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Representations for Phrasal Verbs\n",
    "In this section, I get the BERT, GPT2, and RoBERTa representations for all of the phrasal verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running bert-base-cased for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate BERT Reps\n",
    "vectors_bert, pairs_bert = generate_representations(data['verb_string'], data['sent_string'], layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving BERT Reps\n",
    "with open('bert_vectors_full.np', 'wb') as f:\n",
    "    np.save(f, vectors_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running gpt2 for layer [0, 3, 6, 9, 12] !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 Reps\n",
    "vectors_gpt2, pairs_gpt2 = generate_representations(data['verb_string'], data['sent_string'], model='gpt2', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving GPT2 Reps\n",
    "with open('gpt_vectors_full.np', 'wb') as gpt:\n",
    "    np.save(gpt, vectors_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running roberta-base for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate roberta Reps\n",
    "vectors_roberta, pairs_roberta = generate_representations(data['verb_string'], data['sent_string'], model='roberta-base', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RoBERTa Reps\n",
    "with open('roberta_vectors_full.np', 'wb') as rob:\n",
    "    np.save(rob, vectors_roberta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Representations for Verbs Only\n",
    "In this section, I get the BERT, GPT2, and RoBERTa representations for _only_ the verb in each phrasal verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running bert-base-cased for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate BERT Reps\n",
    "vectors_bert_verb, pairs_bert_verb = generate_representations(data['verb_string_only'], data['sent_string'], layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving BERT Reps\n",
    "with open('bert_vectors_verb.np', 'wb') as bert_verb:\n",
    "    np.save(bert_verb, vectors_bert_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running gpt2 for layer [0, 3, 6, 9, 12] !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 Reps\n",
    "vectors_gpt2_verb, pairs_gpt2_verb = generate_representations(data['verb_string_only'], data['sent_string'], model='gpt2', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving GPT2 Reps\n",
    "with open('gpt_vectors_verb.np', 'wb') as gpt_verb:\n",
    "    np.save(gpt_verb, vectors_gpt2_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running roberta-base for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate RoBERTa Reps\n",
    "vectors_roberta_verb, pairs_roberta_verb = generate_representations(data['verb_string_only'], data['sent_string'], model='roberta-base', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RoBERTa Reps\n",
    "with open('roberta_vectors_verb.np', 'wb') as rob_verb:\n",
    "    np.save(rob_verb, vectors_roberta_verb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Representations for Non-Verbs Only\n",
    "In this section, I get the BERT, GPT2, and RoBERTa representations for everything _except_ the verb in each phrasal verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running bert-base-cased for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate BERT Reps\n",
    "vectors_bert_nonverb, pairs_bert_nonverb = generate_representations(data['non_verbs_only'], data['sent_string'], layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving BERT Reps\n",
    "with open('bert_vectors_nonverb.np', 'wb') as bert_nonverb:\n",
    "    np.save(bert_nonverb, vectors_bert_nonverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running gpt2 for layer [0, 3, 6, 9, 12] !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 Reps\n",
    "vectors_gpt2_nonverb, pairs_gpt2_nonverb = generate_representations(data['non_verbs_only'], data['sent_string'], model='gpt2', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving GPT2 Reps\n",
    "with open('gpt_vectors_nonverb.np', 'wb') as gpt_nonverb:\n",
    "    np.save(gpt_nonverb, vectors_gpt2_nonverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running roberta-base for layer [0, 3, 6, 9, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate RoBERTa Reps\n",
    "vectors_roberta_nonverb, pairs_roberta_nonverb = generate_representations(data['non_verbs_only'], data['sent_string'], model='roberta-base', layer=[0,3,6,9,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RoBERTa Reps\n",
    "with open('roberta_vectors_nonverb.np', 'wb') as rob_nonverb:\n",
    "    np.save(rob_nonverb, vectors_roberta_nonverb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads in some of the representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Reps\n",
    "with open('roberta_vectors_nonverb.np', 'rb') as test_rob:\n",
    "    test_rob_loaded = np.load(test_rob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Function\n",
    "\n",
    "In the following code blocks, I construct a master function that gets embeddings (full phrasal verb, verb only, nonverb only) for all 12 layers of BERT, GPT2, RoBERTa, XLNet, DistilGPT, and DistilBERT. \n",
    "\n",
    "On my laptop (Dell XPS 13 from 2020; no GPUs), the code takes this long to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert-base-cased', 'roberta-base', 'gpt2', 'distilbert-base-cased', 'distilgpt2', 'xlnet-base-cased']\n",
    "embedding_types = ['full_verb', 'verb_only', 'nonverb_only']\n",
    "verb_strings = np.array((data['verb_string'], data['verb_string_only'], data['non_verbs_only']))\n",
    "sent_strings = data['sent_string']\n",
    "layers = [i for i in range(13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_representations(model, embedding_type, verb_strings, sent_strings, layer):\n",
    "    vectors, pairs = generate_representations(verb_strings, sent_strings, model=model, layer=layer)\n",
    "\n",
    "    with open(f'{model}_{embedding_type}_vectors.np', 'wb') as out:\n",
    "        np.save(out, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_pipeline(models=models, embedding_types=embedding_types, verb_strings=verb_strings, sent_strings=sent_strings, layers=layers):\n",
    "    for model in models:\n",
    "        print('LAYERS:', layers)\n",
    "        input_layers = layers\n",
    "        print('INPUT LAYERS BEFORE MODEL CHECK:', input_layers)\n",
    "        if model == 'distilbert-base-cased' or 'distilgpt2':\n",
    "            input_layers = input_layers[:7]\n",
    "        print('INPUT LAYERS AFTER MODEL CHECK:', input_layers)\n",
    "        print(f'################# WORKING ON {model} #################')\n",
    "        for i in range(len(embedding_types)):\n",
    "            print(f'### BUILDING {embedding_types[i]} ###')\n",
    "            build_and_save_representations(model, embedding_types[i], verb_strings[i],\n",
    "                                            sent_strings, input_layers)\n",
    "        print(f'################# {model} COMPLETE #################')\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYERS: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "################# WORKING ON distilgpt2 #################\n",
      "### BUILDING full_verb ###\n",
      "########################################\n",
      "Running distilgpt2 for layer [0, 1, 2, 3, 4, 5, 6] !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete.\n",
      "\n",
      "################# distilgpt2 COMPLETE #################\n",
      "\n",
      "\n",
      "\n",
      "LAYERS: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "################# WORKING ON bert-base-cased #################\n",
      "### BUILDING full_verb ###\n",
      "########################################\n",
      "Running bert-base-cased for layer [0, 1, 2, 3, 4, 5, 6] !\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# The whole shibang!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m representation_pipeline(models\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdistilgpt2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m], embedding_types\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfull_verb\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn [19], line 10\u001b[0m, in \u001b[0;36mrepresentation_pipeline\u001b[1;34m(models, embedding_types, verb_strings, sent_strings, layers)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(embedding_types)):\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m### BUILDING \u001b[39m\u001b[39m{\u001b[39;00membedding_types[i]\u001b[39m}\u001b[39;00m\u001b[39m ###\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     build_and_save_representations(model, embedding_types[i], verb_strings[i],\n\u001b[0;32m     11\u001b[0m                                     sent_strings, input_layers)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m################# \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m COMPLETE #################\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn [13], line 2\u001b[0m, in \u001b[0;36mbuild_and_save_representations\u001b[1;34m(model, embedding_type, verb_strings, sent_strings, layer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_and_save_representations\u001b[39m(model, embedding_type, verb_strings, sent_strings, layer):\n\u001b[1;32m----> 2\u001b[0m     vectors, pairs \u001b[39m=\u001b[39m generate_representations(verb_strings, sent_strings, model\u001b[39m=\u001b[39;49mmodel, layer\u001b[39m=\u001b[39;49mlayer)\n\u001b[0;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00membedding_type\u001b[39m}\u001b[39;00m\u001b[39m_vectors.np\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m out:\n\u001b[0;32m      5\u001b[0m         np\u001b[39m.\u001b[39msave(out, vectors)\n",
      "File \u001b[1;32mc:\\Users\\johns\\Documents\\grad_school\\research\\light-verb-construction-embeddings\\miniconsbatched.py:37\u001b[0m, in \u001b[0;36mgenerate_representations\u001b[1;34m(words, contexts, model, batch_size, device, layer)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# vectors.extend(model.extract_representation(pairs[ix_range[0]: ix_range[1]], layer=layer).numpy())\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39m# Multiple layers\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(layer)\u001b[39m!=\u001b[39m\u001b[39mint\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     reps \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mextract_representation(pairs[ix_range[\u001b[39m0\u001b[39;49m]: ix_range[\u001b[39m1\u001b[39;49m]], layer\u001b[39m=\u001b[39;49mlayer)\n\u001b[0;32m     38\u001b[0m     reps \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([layer_reps\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m layer_reps \u001b[39min\u001b[39;00m reps])\n\u001b[0;32m     39\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(reps)):\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\minicons\\cwe.py:147\u001b[0m, in \u001b[0;36mCWE.extract_representation\u001b[1;34m(self, sentence_words, layer)\u001b[0m\n\u001b[0;32m    143\u001b[0m sentences \u001b[39m=\u001b[39m [sentence_words] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sentence_words[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m sentence_words\n\u001b[0;32m    145\u001b[0m num_inputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentences)\n\u001b[1;32m--> 147\u001b[0m input_ids, hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_text(\u001b[39mlist\u001b[39;49m(\u001b[39mlist\u001b[39;49m(\u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49msentences))[\u001b[39m0\u001b[39;49m]), layer)\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sentences[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[0;32m    150\u001b[0m     sentences \u001b[39m=\u001b[39m [(s, character_span(s, w)) \u001b[39mfor\u001b[39;00m s, w \u001b[39min\u001b[39;00m sentences]\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\minicons\\cwe.py:89\u001b[0m, in \u001b[0;36mCWE.encode_text\u001b[1;34m(self, text, layer)\u001b[0m\n\u001b[0;32m     86\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m attention_mask\n\u001b[0;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m \u001b[39m# Compute hidden states for the sentence for the given layer.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded)\n\u001b[0;32m     91\u001b[0m     \u001b[39m# Hidden states appear as the last element of the otherwise custom hidden_states object\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, \u001b[39mlist\u001b[39m) \u001b[39mor\u001b[39;00m layer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1020\u001b[0m )\n\u001b[1;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1022\u001b[0m     embedding_output,\n\u001b[0;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    494\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    497\u001b[0m         hidden_states,\n\u001b[0;32m    498\u001b[0m         attention_mask,\n\u001b[0;32m    499\u001b[0m         head_mask,\n\u001b[0;32m    500\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    501\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    503\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    505\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    417\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    418\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 426\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    427\u001b[0m         hidden_states,\n\u001b[0;32m    428\u001b[0m         attention_mask,\n\u001b[0;32m    429\u001b[0m         head_mask,\n\u001b[0;32m    430\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    431\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    432\u001b[0m         past_key_value,\n\u001b[0;32m    433\u001b[0m         output_attentions,\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    436\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:354\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    351\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[0;32m    353\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    356\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mc:\\Users\\johns\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1839\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1841\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1842\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The whole shibang!\n",
    "representation_pipeline(models=['distilgpt2', 'bert-base-cased'], embedding_types=['full_verb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Running roberta-base for layer [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] !\n",
      "Run complete.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99e36c4e7668e780a07345726bda8c16f8801d43d9a537c65be6dd0ae0350df0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
